{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Brain import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from Env import create_env\n",
    "from Hype import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_env('Breakout')\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "tb = SummaryWriter('runs/breakout/Second_double_shrinked_buffer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'Trained_param/Breakout/DoubleDQN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_net = TargetNetworkBrain(env).cuda()\n",
    "double_net = DoubleDQNBrain(env).cuda()\n",
    "# dueling_net = DuelingNetworkBrain(env).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_optim = optim.RMSprop(target_net.learnable(), lr=LEARNING_RATE, momentum=GRADIENT_MOMENTUM, alpha=SQUARED_GRADIENT_MOMENTUM, eps=MIN_SQUARED_GRADIENT)\n",
    "double_optim = optim.RMSprop(double_net.learnable(), lr=LEARNING_RATE, momentum=GRADIENT_MOMENTUM, alpha=SQUARED_GRADIENT_MOMENTUM, eps=MIN_SQUARED_GRADIENT)\n",
    "# dueling_optim = optim.RMSprop(dueling_net.learnable(), lr=LEARNING_RATE, momentum=GRADIENT_MOMENTUM, alpha=SQUARED_GRADIENT_MOMENTUM, eps=MIN_SQUARED_GRADIENT)\n",
    "criterian = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple( 'Transition', ('s1', 'a1', 'r1', 's2', 'done') )\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = ReplayMemory(REPLAY_MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = env.reset()\n",
    "for step in range(REPLAY_START_SIZE):\n",
    "# for step in range(33):\n",
    "    a1 = env.action_space.sample()\n",
    "    s2, r1, done, info = env.step(a1)\n",
    "    replay_memory.push(s1, a1, r1, s2, done)\n",
    "    if done:\n",
    "        s1 = env.reset()\n",
    "    else:\n",
    "        s1 = s2\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "for episode in range(1, FINAL_EXPLORATION_FRAME+1):\n",
    "# for episode in range(1, 300+1):\n",
    "    print(f'Episode: {episode}')\n",
    "    total_reward = 0\n",
    "    total_target_loss = 0\n",
    "    total_double_loss = 0\n",
    "    total_dueling_loss = 0\n",
    "    s1 = env.reset()\n",
    "    while True:\n",
    "        step += 1\n",
    "        # env.render()\n",
    "        epsilon = np.interp(step, [0, FINAL_EXPLORATION_FRAME], [INITIAL_EXPLORATION, FINAL_EXPLORATION])\n",
    "        if epsilon > random.random():\n",
    "            a1 = env.action_space.sample()\n",
    "        else:\n",
    "            a1 = double_net.get_action(phi(s1))\n",
    "        s2, r1, done, info = env.step(a1)\n",
    "        total_reward += r1\n",
    "        replay_memory.push(s1, a1, r1, s2, done)\n",
    "        s1 = s2\n",
    "        if step % UPDATE_FREQUENCY == 0: # update\n",
    "            transitions = replay_memory.sample(MINIBATCH_SIZE)\n",
    "            s1_np = np.asarray([(t.s1) for t in transitions])\n",
    "            a1_np = np.asarray([(t.a1) for t in transitions])\n",
    "            r1_np = np.asarray([(t.r1) for t in transitions]).astype('float32')\n",
    "            s2_np = np.asarray([(t.s2) for t in transitions])\n",
    "            done_np = np.asarray([(t.done) for t in transitions])\n",
    "            # y_target = (torch.as_tensor(r1_np)).cuda() + (torch.as_tensor(1 - done_np)).cuda() * DISCOUNT_FACTOR * target_net.get_td(phi(s2_np))\n",
    "            y_double = (torch.as_tensor(r1_np)).cuda() + (torch.as_tensor(1 - done_np)).cuda() * DISCOUNT_FACTOR * double_net.get_td(phi(s2_np))\n",
    "            # y_dueling = (torch.as_tensor(r1_np)).cuda() + (torch.as_tensor(1 - done_np)).cuda() * DISCOUNT_FACTOR * dueling_net.get_td(phi(s2_np))\n",
    "            # q_target = target_net.get_Q(phi(s1_np), phi(a1_np).long())\n",
    "            q_double = double_net.get_Q(phi(s1_np), phi(a1_np).long())\n",
    "            # q_dueling = dueling_net.get_Q(phi(s1_np), phi(a1_np).long())\n",
    "            # loss_target = criterian(y_target, q_target)\n",
    "            loss_double = criterian(y_double, q_double)\n",
    "            # loss_dueling = criterian(y_dueling, q_dueling)\n",
    "            # target_optim.zero_grad()\n",
    "            double_optim.zero_grad()\n",
    "            # dueling_optim.zero_grad()\n",
    "            # loss_target.backward()\n",
    "            loss_double.backward()\n",
    "            # loss_dueling.backward()\n",
    "            # target_optim.step()\n",
    "            double_optim.step()\n",
    "            # dueling_optim.step()\n",
    "            # total_target_loss += loss_target.item()\n",
    "            total_double_loss += loss_double.item()\n",
    "            # total_dueling_loss += loss_dueling.item()\n",
    "        if step % TARGET_NETWORK_UPDATE_FREQUENCY == 0:\n",
    "            print('Updating Target net', step)\n",
    "            # target_net.update()\n",
    "            double_net.update()\n",
    "        if done:\n",
    "            tb.add_scalar('Loss', total_double_loss, episode)\n",
    "            # tb.add_scalar('Double Net Loss', total_double_loss, episode)\n",
    "            # tb.add_scalar('Dueling Net Loss', total_dueling_loss, episode)\n",
    "            print(f'Step = {step}')\n",
    "            print(f'Score = {total_reward}')\n",
    "            tb.add_scalar('Score', total_reward, episode)\n",
    "            break\n",
    "    if episode <= 500:\n",
    "        if episode % 50 == 0:\n",
    "            torch.save(double_net.state_dict(), f'{save_path}{episode}_trains.dqn')\n",
    "    elif episode <= 1000:\n",
    "        if episode % 100 == 0:\n",
    "            torch.save(double_net.state_dict(), f'{save_path}{episode}_trains.dqn')\n",
    "    elif episode <= 5000:\n",
    "        if episode % 500 == 0:\n",
    "            torch.save(double_net.state_dict(), f'{save_path}{episode}_trains.dqn')\n",
    "    elif episode <= 10000:\n",
    "        if episode % 1000 == 0:\n",
    "            torch.save(double_net.state_dict(), f'{save_path}{episode}_trains.dqn')\n",
    "    elif episode <= 50000:\n",
    "        if episode % 5000 == 0:\n",
    "            torch.save(double_net.state_dict(), f'{save_path}{episode}_trains.dqn')\n",
    "    else:\n",
    "        if episode % 10000 == 0:\n",
    "            torch.save(double_net.state_dict(), f'{save_path}{episode}_trains.dqn')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 試玩一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        env.render()\n",
    "        print('steping', end=' ')\n",
    "        action = dueling_net.get_action(phi(state))\n",
    "        print(action)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    env.close()\n",
    "    print(total_reward)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18b98b91350c6e0672ce8ac96b736ee962e9ad13b13604e5485807acd250ffcf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('env11': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
